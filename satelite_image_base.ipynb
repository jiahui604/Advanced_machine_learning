{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/software/python-anaconda-2020.02-el7-x86_64/lib/python3.7/site-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.25.9) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n",
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: failed with initial frozen solve. Retrying with flexible solve.\n",
      "Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /software/python-anaconda-2020.02-el7-x86_64\n",
      "\n",
      "  added / updated specs:\n",
      "    - tensorflow\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    _tflow_select-2.3.0        |              mkl           2 KB\n",
      "    absl-py-0.10.0             |           py37_0         169 KB\n",
      "    aiohttp-3.6.3              |   py37h7b6447c_0         544 KB\n",
      "    astunparse-1.6.3           |             py_0          17 KB\n",
      "    async-timeout-3.0.1        |           py37_0          12 KB\n",
      "    blinker-1.4                |           py37_0          22 KB\n",
      "    c-ares-1.16.1              |       h7b6447c_0         108 KB\n",
      "    ca-certificates-2020.10.14 |                0         121 KB\n",
      "    cachetools-4.1.1           |             py_0          12 KB\n",
      "    conda-4.9.0                |           py37_0         2.9 MB\n",
      "    gast-0.3.3                 |             py_0          14 KB\n",
      "    google-auth-1.22.1         |             py_0          61 KB\n",
      "    google-auth-oauthlib-0.4.1 |             py_2          20 KB\n",
      "    grpcio-1.31.0              |   py37hf8bcb03_0         2.0 MB\n",
      "    keras-preprocessing-1.1.0  |             py_1          37 KB\n",
      "    libprotobuf-3.13.0.1       |       hd408876_0         2.0 MB\n",
      "    markdown-3.3.2             |           py37_0         127 KB\n",
      "    multidict-4.7.6            |   py37h7b6447c_1          65 KB\n",
      "    oauthlib-3.1.0             |             py_0          91 KB\n",
      "    opt_einsum-3.1.0           |             py_0          54 KB\n",
      "    protobuf-3.13.0.1          |   py37he6710b0_1         634 KB\n",
      "    pyasn1-0.4.8               |             py_0          57 KB\n",
      "    pyasn1-modules-0.2.8       |             py_0          72 KB\n",
      "    pyjwt-1.7.1                |           py37_0          33 KB\n",
      "    requests-oauthlib-1.3.0    |             py_0          23 KB\n",
      "    rsa-4.6                    |             py_0          26 KB\n",
      "    tensorboard-2.2.1          |     pyh532a8cf_0         2.4 MB\n",
      "    tensorboard-plugin-wit-1.6.0|             py_0         630 KB\n",
      "    tensorflow-2.2.0           |mkl_py37h6e9ce2d_0           4 KB\n",
      "    tensorflow-base-2.2.0      |mkl_py37hd506778_0       129.6 MB\n",
      "    tensorflow-estimator-2.2.0 |     pyh208ff02_0         254 KB\n",
      "    termcolor-1.1.0            |           py37_1           8 KB\n",
      "    yarl-1.6.2                 |   py37h7b6447c_0         134 KB\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:       142.1 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  _tflow_select      pkgs/main/linux-64::_tflow_select-2.3.0-mkl\n",
      "  absl-py            pkgs/main/linux-64::absl-py-0.10.0-py37_0\n",
      "  aiohttp            pkgs/main/linux-64::aiohttp-3.6.3-py37h7b6447c_0\n",
      "  astunparse         pkgs/main/noarch::astunparse-1.6.3-py_0\n",
      "  async-timeout      pkgs/main/linux-64::async-timeout-3.0.1-py37_0\n",
      "  blinker            pkgs/main/linux-64::blinker-1.4-py37_0\n",
      "  c-ares             pkgs/main/linux-64::c-ares-1.16.1-h7b6447c_0\n",
      "  cachetools         pkgs/main/noarch::cachetools-4.1.1-py_0\n",
      "  gast               pkgs/main/noarch::gast-0.3.3-py_0\n",
      "  google-auth        pkgs/main/noarch::google-auth-1.22.1-py_0\n",
      "  google-auth-oauth~ pkgs/main/noarch::google-auth-oauthlib-0.4.1-py_2\n",
      "  google-pasta       pkgs/main/noarch::google-pasta-0.2.0-py_0\n",
      "  grpcio             pkgs/main/linux-64::grpcio-1.31.0-py37hf8bcb03_0\n",
      "  keras-preprocessi~ pkgs/main/noarch::keras-preprocessing-1.1.0-py_1\n",
      "  libprotobuf        pkgs/main/linux-64::libprotobuf-3.13.0.1-hd408876_0\n",
      "  markdown           pkgs/main/linux-64::markdown-3.3.2-py37_0\n",
      "  multidict          pkgs/main/linux-64::multidict-4.7.6-py37h7b6447c_1\n",
      "  oauthlib           pkgs/main/noarch::oauthlib-3.1.0-py_0\n",
      "  opt_einsum         pkgs/main/noarch::opt_einsum-3.1.0-py_0\n",
      "  protobuf           pkgs/main/linux-64::protobuf-3.13.0.1-py37he6710b0_1\n",
      "  pyasn1             pkgs/main/noarch::pyasn1-0.4.8-py_0\n",
      "  pyasn1-modules     pkgs/main/noarch::pyasn1-modules-0.2.8-py_0\n",
      "  pyjwt              pkgs/main/linux-64::pyjwt-1.7.1-py37_0\n",
      "  requests-oauthlib  pkgs/main/noarch::requests-oauthlib-1.3.0-py_0\n",
      "  rsa                pkgs/main/noarch::rsa-4.6-py_0\n",
      "  tensorboard        pkgs/main/noarch::tensorboard-2.2.1-pyh532a8cf_0\n",
      "  tensorboard-plugi~ pkgs/main/noarch::tensorboard-plugin-wit-1.6.0-py_0\n",
      "  tensorflow         pkgs/main/linux-64::tensorflow-2.2.0-mkl_py37h6e9ce2d_0\n",
      "  tensorflow-base    pkgs/main/linux-64::tensorflow-base-2.2.0-mkl_py37hd506778_0\n",
      "  tensorflow-estima~ pkgs/main/noarch::tensorflow-estimator-2.2.0-pyh208ff02_0\n",
      "  termcolor          pkgs/main/linux-64::termcolor-1.1.0-py37_1\n",
      "  yarl               pkgs/main/linux-64::yarl-1.6.2-py37h7b6447c_0\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  ca-certificates                               2020.6.24-0 --> 2020.10.14-0\n",
      "  conda                                        4.8.3-py37_0 --> 4.9.0-py37_0\n",
      "  openssl                                 1.1.1g-h7b6447c_0 --> 1.1.1h-h7b6447c_0\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "tensorboard-2.2.1    | 2.4 MB    | ##################################### | 100% \n",
      "yarl-1.6.2           | 134 KB    | ##################################### | 100% \n",
      "requests-oauthlib-1. | 23 KB     | ##################################### | 100% \n",
      "oauthlib-3.1.0       | 91 KB     | ##################################### | 100% \n",
      "pyjwt-1.7.1          | 33 KB     | ##################################### | 100% \n",
      "absl-py-0.10.0       | 169 KB    | ##################################### | 100% \n",
      "tensorflow-2.2.0     | 4 KB      | ##################################### | 100% \n",
      "tensorboard-plugin-w | 630 KB    | ##################################### | 100% \n",
      "cachetools-4.1.1     | 12 KB     | ##################################### | 100% \n",
      "grpcio-1.31.0        | 2.0 MB    | ##################################### | 100% \n",
      "pyasn1-0.4.8         | 57 KB     | ##################################### | 100% \n",
      "async-timeout-3.0.1  | 12 KB     | ##################################### | 100% \n",
      "pyasn1-modules-0.2.8 | 72 KB     | ##################################### | 100% \n",
      "multidict-4.7.6      | 65 KB     | ##################################### | 100% \n",
      "tensorflow-base-2.2. | 129.6 MB  | ##################################### | 100% \n",
      "keras-preprocessing- | 37 KB     | ##################################### | 100% \n",
      "aiohttp-3.6.3        | 544 KB    | ##################################### | 100% \n",
      "_tflow_select-2.3.0  | 2 KB      | ##################################### | 100% \n",
      "blinker-1.4          | 22 KB     | ##################################### | 100% \n",
      "markdown-3.3.2       | 127 KB    | ##################################### | 100% \n",
      "protobuf-3.13.0.1    | 634 KB    | ##################################### | 100% \n",
      "conda-4.9.0          | 2.9 MB    | ##################################### | 100% \n",
      "opt_einsum-3.1.0     | 54 KB     | ##################################### | 100% \n",
      "rsa-4.6              | 26 KB     | ##################################### | 100% \n",
      "gast-0.3.3           | 14 KB     | ##################################### | 100% \n",
      "google-auth-1.22.1   | 61 KB     | ##################################### | 100% \n",
      "termcolor-1.1.0      | 8 KB      | ##################################### | 100% \n",
      "c-ares-1.16.1        | 108 KB    | ##################################### | 100% \n",
      "google-auth-oauthlib | 20 KB     | ##################################### | 100% \n",
      "ca-certificates-2020 | 121 KB    | ##################################### | 100% \n",
      "tensorflow-estimator | 254 KB    | ##################################### | 100% \n",
      "astunparse-1.6.3     | 17 KB     | ##################################### | 100% \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "libprotobuf-3.13.0.1 | 2.0 MB    | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: failed\n",
      "\n",
      "EnvironmentNotWritableError: The current user does not have write permissions to the target environment.\n",
      "  environment location: /software/python-anaconda-2020.02-el7-x86_64\n",
      "  uid: 1179063792\n",
      "  gid: 1179063792\n",
      "\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-90b3d5bab35c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;31m#modeling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplot_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "# image processing\n",
    "import numpy as np\n",
    "import imageio\n",
    "import tifffile as tiff\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "import random\n",
    "import math\n",
    "import tensorflow\n",
    "#modeling \n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D,\\\n",
    "                                    concatenate, Conv2DTranspose\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import CSVLogger\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = (10,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/home/jliu0604/AML/satelite_image/data'\n",
    "PATCH_SIZE = 160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test reading image\n",
    "img = tiff.imread(data_path + '/gt_mband/01.tif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(img[1,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tiff.imshow(img[(3,2,1),:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# superimpose mask on to the satelite images\n",
    "st_img = tiff.imread(data_path + '/mband/01.tif')\n",
    "mask = tiff.imread(data_path + '/gt_mband/01.tif')\n",
    "\n",
    "\n",
    "def superimpose_stlite_mask(st_img, mask, color = (10,0,0)):\n",
    "        #normalize image and select only three channels to display\n",
    "        st_normed = 255.0 * st_img / st_img.max()\n",
    "        # create color mask using RGB channles\n",
    "        colored_mask = np.stack([mask*color[0], mask*color[1], mask*color[2]])\n",
    "        # combine the colored_mask and st_img together\n",
    "        combined = (st_normed + colored_mask).clip(0,255).astype(np.uint8)\n",
    "        return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = superimpose_stlite_mask(st_img[(4,2,1),:,:], mask[0,:,:])\n",
    "tiff.imshow(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize the image\n",
    "Normalize image to make each pixel within the range of [-1,1]\n",
    "\n",
    "Since we need to compare the input image ð¼ to the output image ð¼Ë†, it should be readily possible to enforce the pixel values of ð¼Ë† into a simple, known, hard range. Using sigmoid produces values in [0,1], while using tanh does so in [âˆ’1,1]. However, it is often thought that that tanh is better than sigmoid; e.g.,\n",
    "\n",
    "https://stats.stackexchange.com/questions/142348/tanh-vs-sigmoid-in-neural-net\n",
    "\n",
    "https://stats.stackexchange.com/questions/330559/why-is-tanh-almost-always-better-than-sigmoid-as-an-activation-function/369538\n",
    "\n",
    "https://stats.stackexchange.com/questions/101560/tanh-activation-function-vs-sigmoid-activation-function\n",
    "\n",
    "In other words, for cases where the output must match the input, using [âˆ’1,1] may be a better choice. Furthermore, though not \"standardized\", the range [âˆ’1,1] is still zero-centered (unlike [0,1]), which is easier for the network to learn to standardize (though I suspect this matters only rather early in training). https://datascience.stackexchange.com/questions/54296/should-input-images-be-normalized-to-1-to-1-or-0-to-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_images(img):\n",
    "    normalized = 2 * (img - img.min()) / (img.max() - img.min()) - 1\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_img.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_images(st_img).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating extented images\n",
    "Since it's difficult for CNN to train on the image edges, creating extended edges will help on training. We will fill the extended edges using mirror images. So far, just extended the right and bottom part, but this can be further improved by extend all four edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creat extented x and mask\n",
    "# def create_extended_imgs(x, mask, PATCH_SIZE = PATCH_SIZE):\n",
    "#     img_height = x.shape[1]\n",
    "#     img_width = x.shape[2]\n",
    "#     n_channels = x.shape[0]\n",
    "#     n_classes = mask.shape[0]\n",
    "#     # make extended image with mirror filler\n",
    "#     npatches_vertical = math.ceil(img_height / PATCH_SIZE)\n",
    "#     npatches_horizontal = math.ceil(img_width / PATCH_SIZE)\n",
    "#     ext_x = np.zeros(shape = (n_channels, npatches_vertical*PATCH_SIZE, npatches_horizontal*PATCH_SIZE), dtype = np.float32)\n",
    "#     ext_m = np.zeros(shape = (n_classes, npatches_vertical*PATCH_SIZE, npatches_horizontal*PATCH_SIZE), dtype = np.float32)\n",
    "#     #print(ext_x.shape)\n",
    "#     # fill extended image with mirror filler\n",
    "#     ext_x[:,:img_height, :img_width] = x\n",
    "#     ext_m[:,:img_height, :img_width] = mask\n",
    "#     for i in range(img_height, ext_x.shape[1]):\n",
    "#         #print(x[:,2*img_height - i -1, :])\n",
    "#         #can't use ext_x[:,i,:] = x[:,2*img_height - i -1, :], since x, ext_x shape is different\n",
    "#         ext_x[:,i,:] = ext_x[:,2*img_height - i -1, :]\n",
    "#         ext_m[:,i,:] = ext_m[:,2*img_height - i -1, :]\n",
    "#     for i in range(img_width, ext_x.shape[2]):\n",
    "#         ext_x[:,:,i] = ext_x[:,:,2*img_width -i -1]\n",
    "#         ext_m[:,:,i] = ext_m[:,:,2*img_width -i -1]\n",
    "\n",
    "#     return ext_x, ext_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creat extented x and mask\n",
    "def create_extended_imgs(x, mask = None, PATCH_SIZE = PATCH_SIZE):\n",
    "    img_height = x.shape[1]\n",
    "    img_width = x.shape[2]\n",
    "    n_channels = x.shape[0]\n",
    "  \n",
    "    # make extended image with mirror filler\n",
    "    npatches_vertical = math.ceil(img_height / PATCH_SIZE)\n",
    "    npatches_horizontal = math.ceil(img_width / PATCH_SIZE)\n",
    "    ext_x = np.zeros(shape = (n_channels, npatches_vertical*PATCH_SIZE, npatches_horizontal*PATCH_SIZE), dtype = np.float32)\n",
    "  \n",
    "    #print(ext_x.shape)\n",
    "    # fill extended image with mirror filler\n",
    "    ext_x[:,:img_height, :img_width] = x\n",
    "   \n",
    "    for i in range(img_height, ext_x.shape[1]):\n",
    "        #print(x[:,2*img_height - i -1, :])\n",
    "        #can't use ext_x[:,i,:] = x[:,2*img_height - i -1, :], since x, ext_x shape is different\n",
    "        ext_x[:,i,:] = ext_x[:,2*img_height - i -1, :]\n",
    "    for i in range(img_width, ext_x.shape[2]):\n",
    "        ext_x[:,:,i] = ext_x[:,:,2*img_width -i -1]\n",
    "\n",
    "    if mask is not None:\n",
    "        n_classes = mask.shape[0]\n",
    "        ext_m = np.zeros(shape = (n_classes, npatches_vertical*PATCH_SIZE, npatches_horizontal*PATCH_SIZE), dtype = np.float32)\n",
    "        ext_m[:,:img_height, :img_width] = mask\n",
    "        for i in range(img_height, ext_x.shape[1]):\n",
    "        #print(x[:,2*img_height - i -1, :])\n",
    "        #can't use ext_x[:,i,:] = x[:,2*img_height - i -1, :], since x, ext_x shape is different\n",
    "            ext_m[:,i,:] = ext_m[:,2*img_height - i -1, :]\n",
    "        for i in range(img_width, ext_x.shape[2]):\n",
    "            ext_m[:,:,i] = ext_m[:,:,2*img_width -i -1]\n",
    "        return ext_x, ext_m\n",
    "    \n",
    "    return ext_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filled_img, filled_mask = create_extended_imgs(st_img, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tiff.imshow(filled_img[(4,2,1),:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiff.imshow(filled_mask[(4,2,1),:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating random patches\n",
    "Our image is 837*851, it's usually too large for the network to train, so we will create patches. However, one concern with creating pathes is we might lose information on cropped image edges, so will creating mirror images for the edges.\n",
    "\n",
    "Since we can absolutely crop images side by side and creating mirror images on the edges, this might also put us in risk of lossing information on edges. So another way to optimize it is to randomly cropping over an image, that way some of the patches will overlap with each other and get trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_patches(img, mask, PATCH_SIZE = PATCH_SIZE):\n",
    "    assert len(img.shape) == 3 and img.shape[1] > PATCH_SIZE\n",
    "    assert img.shape[2] > PATCH_SIZE \n",
    "    assert img.shape[1:3] == mask.shape[1:3]\n",
    "    \n",
    "    xc = random.randint(0, img.shape[2] - PATCH_SIZE)\n",
    "    yc = random.randint(0, img.shape[1] - PATCH_SIZE)\n",
    "    \n",
    "    new_img = img[:, yc : (yc + PATCH_SIZE), xc: (xc + PATCH_SIZE)]\n",
    "    new_mask = mask[:, yc : (yc + PATCH_SIZE), xc: (xc + PATCH_SIZE)]\n",
    "    \n",
    "    return new_img, new_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_img, pathc_mask = get_random_patches(st_img, mask, 160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(patch_img[5,:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to generate random patches for the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_patches(img, mask, n_patches, PATCH_SIZE):\n",
    "    xs = list()\n",
    "    ys = list()\n",
    "    \n",
    "    total_patches = 0\n",
    "    while total_patches < n_patches:\n",
    "        img_patch, mask_patch = get_random_patches(img, mask, PATCH_SIZE)\n",
    "        xs.append(img_patch)\n",
    "        ys.append(mask_patch)\n",
    "        total_patches += 1\n",
    "    print('generated {} pacthes images'.format(total_patches))\n",
    "    return np.array(xs), np.array(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, ys = get_patches(st_img,mask,10,160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we have a list of images, put them into dictionary, random sampling from the list\n",
    "def get_patches_batch(x_dict, y_dict, n_patches, sz=PATCH_SIZE):\n",
    "    x = list()\n",
    "    y = list()\n",
    "    \n",
    "    n_imgs = len(x_dict)\n",
    "    # make sure each image will get sampled\n",
    "    sub_npatches = n_patches // n_imgs\n",
    "    left_npatches = n_patches % n_imgs\n",
    "    \n",
    "    total_patches = 0\n",
    "    \n",
    "\n",
    "    for i in list(x_dict.keys()):\n",
    "        while total_patches < int(i)*sub_npatches:\n",
    "            img = x_dict[i]\n",
    "            mask = y_dict[i]\n",
    "\n",
    "            img_patch, mask_patch = get_random_patches(img, mask, PATCH_SIZE)\n",
    "            x.append(img_patch)\n",
    "            y.append(mask_patch)\n",
    "\n",
    "            total_patches += 1\n",
    "    # The rest images will be filled from image 01        \n",
    "    while total_patches < n_patches:\n",
    "        img_patch, mask_patch = get_random_patches(x_dict['01'], y_dict['01'], PATCH_SIZE)\n",
    "        x.append(img_patch)\n",
    "        y.append(mask_patch)\n",
    "\n",
    "        \n",
    "        total_patches += 1\n",
    "    print('Generated {} patches'.format(total_patches))\n",
    "    return np.array(x), np.array(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train,y_train=get_patches_batch(X_DICT_TRAIN,Y_DICT_TRAIN, 100, 160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train.shape,y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,11):\n",
    "    plt.subplot(2,5,i)\n",
    "    plt.imshow(xs[i-1][4,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating predictions\n",
    "The prediction will also be doing on the extended images, since this will helps on predicting the edges. After that, will crop the extended area. Similarly, this is base on only right and bottom edge mirror extended prediction. Further imporvement can be achieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(img, model, PATCH_SIZE = PATCH_SIZE, n_classes = 5):\n",
    "    ext_x = create_extended_imgs(img, mask = None)\n",
    "    img_height = img.shape[1]\n",
    "    img_width = img.shape[2]\n",
    "    npatches_vertical = int(ext_x.shape[1] / PATCH_SIZE)\n",
    "    npatches_horizontal = int(ext_x.shape[2] / PATCH_SIZE)\n",
    "    \n",
    "    # assemble all patches into one array\n",
    "    patches_list = []\n",
    "    for i in range(0, npatches_vertical):\n",
    "        for j in range(0, npatches_horizontal):\n",
    "            x0, x1 = i*PATCH_SIZE, (i+1)*PATCH_SIZE\n",
    "            y0, y1 = j*PATCH_SIZE, (j+1)*PATCH_SIZE\n",
    "            \n",
    "            patches_list.append(ext_x[:,y0:y1,x0:x1])\n",
    "    patches_array = np.asarray(patches_list)\n",
    "    patches_array = patches_array.transpose([0,2,3,1])\n",
    "    print(patches_array.shape)\n",
    "    #return patches_array\n",
    "\n",
    "# predictions:\n",
    "    #patches_predict = patches_array[:,:,:,(1,2,3,4,5)]\n",
    "    #print(patches_predict.shape)\n",
    "    patches_predict = model.predict(patches_array, batch_size=4)\n",
    "    prediction = np.zeros(shape=(ext_x.shape[1], ext_x.shape[2], n_classes), dtype=np.float32)\n",
    "    for k in range(patches_predict.shape[0]):\n",
    "        print(k)\n",
    "        i = k // npatches_horizontal\n",
    "        j = k % npatches_vertical\n",
    "        x0, x1 = i * PATCH_SIZE, (i + 1) * PATCH_SIZE\n",
    "        y0, y1 = j * PATCH_SIZE, (j + 1) * PATCH_SIZE\n",
    "        print(x0,x1,y0,y1)\n",
    "        prediction[y0:y1, x0:x1, :] = patches_predict[k,:, :, :]\n",
    "    return prediction[:img_height, :img_width, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "5%36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#model = 1\n",
    "#ptest = prediction(test_image, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filled_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ptest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tiff.imshow(ptest[:,:,(4,2,1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tiff.imshow(test_image[4,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_unet(n_classes = 5, im_size = PATCH_SIZE, n_channels = 8, n_filters_start =4, growth_factor =2):\n",
    "    n_filters = n_filters_start\n",
    "    \n",
    "    # create model using functional API. It is generally recommend to use the functional layer API via Input, (which creates an InputLayer) without directly using InputLayer.\n",
    "    inputs = Input((im_size, im_size, n_channels))\n",
    "    conv1 = Conv2D(n_filters, (3,3), padding = 'same', activation = 'relu')(inputs)\n",
    "    pool1 = MaxPooling2D((2,2))(conv1)\n",
    "    n_filters *= growth_factor\n",
    "    conv2 = Conv2D(n_filters, (3,3), padding = 'same', activation = 'relu')(pool1)\n",
    "    \n",
    "    n_filters //= growth_factor\n",
    "    upconv = Conv2DTranspose(n_filters, (2,2), strides = (2,2), padding = 'same')(conv2)\n",
    "    concat = concatenate([conv1, upconv])\n",
    "    conv3 = Conv2D(n_filters, (3,3), activation = 'relu', padding = 'same')(concat)\n",
    "    output = Conv2D(n_classes, (1,1), activation = 'sigmoid')(conv3)\n",
    "    model = Model(inputs = inputs, outputs = output)\n",
    "    \n",
    "    # compiling model\n",
    "    model.compile(optimizer = Adam(), loss = 'binary_crossentropy')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_unet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, show_shapes=True)\n",
    "plt.imshow(imageio.imread('simple_unet.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainging on model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_ids = [str(i).zfill(2) for i in range(1, 25)] \n",
    "\n",
    "X_DICT_TRAIN = dict()\n",
    "Y_DICT_TRAIN = dict()\n",
    "X_DICT_VALIDATION = dict()\n",
    "Y_DICT_VALIDATION = dict()\n",
    "\n",
    "print('Reading images')\n",
    "for img_id in img_ids:\n",
    "    img_m = normalize_images(tiff.imread('/home/jliu0604/AML/satelite_image/data/mband/{}.tif'.format(img_id)))\n",
    "    # use mask to / 255 put it in the range of [0,1], the end result will be multiply wit 255\n",
    "    mask = tiff.imread('/home/jliu0604/AML/satelite_image/data/gt_mband/{}.tif'.format(img_id)) / 255\n",
    "    train_xsz = int(3/4 * img_m.shape[1])  # use 75% of image as train and 25% for validation\n",
    "    X_DICT_TRAIN[img_id] = img_m[:,:train_xsz, :]\n",
    "    Y_DICT_TRAIN[img_id] = mask[:,:train_xsz, :]\n",
    "    X_DICT_VALIDATION[img_id] = img_m[:,:train_xsz, :]\n",
    "    Y_DICT_VALIDATION[img_id] = mask[:,:train_xsz, :]\n",
    "    print(img_id + ' read')\n",
    "print('Images were read')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = normalize_images(tiff.imread('/home/jliu0604/AML/satelite_image/data/mband/test.tif'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if the data has been normalized, this is channel first\n",
    "X_DICT_TRAIN['01'].max(),X_DICT_TRAIN['01'].min() ,X_DICT_TRAIN['01'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get random patches for train, test\n",
    "TRAINING_TOTAL = 100\n",
    "VALIDATION_TOTAL = 50\n",
    "x_train, y_train = get_patches_batch(X_DICT_TRAIN, Y_DICT_TRAIN, n_patches=TRAINING_TOTAL, sz=PATCH_SIZE)\n",
    "x_val, y_val = get_patches_batch(X_DICT_VALIDATION, Y_DICT_VALIDATION, n_patches=VALIDATION_TOTAL, sz=PATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transpose images to make channel last\n",
    "x_val = x_val.transpose([0,2,3,1])#\n",
    "x_val.shape\n",
    "y_val = y_val.transpose([0,2,3,1])\n",
    "x_train = x_train.transpose([0,2,3,1])\n",
    "y_train = y_train.transpose([0,2,3,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now training the model:\n",
    "N_EPOCHS = 100\n",
    "BATCH_SIZE = 32\n",
    "# ask Keras to save best weights (in terms of validation loss) into file:\n",
    "model_checkpoint = ModelCheckpoint(filepath='weights_simple_unet.hdf5', monitor='val_loss', save_best_only=True)\n",
    "# ask Keras to log each epoch loss:\n",
    "csv_logger = CSVLogger('log.csv', append=True, separator=';')\n",
    "# ask Keras to log info in TensorBoard format:\n",
    "tensorboard = TensorBoard(log_dir='tensorboard_simple_unet/', write_graph=True, write_images=True)\n",
    "# Fit:\n",
    "model.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=N_EPOCHS,\n",
    "          verbose=0, shuffle=True,\n",
    "          callbacks=[model_checkpoint, csv_logger, tensorboard],\n",
    "          validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_mask = prediction(test_image, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard.notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir=tensorboard_unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
